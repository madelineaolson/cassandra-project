{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9553627-6e60-44fa-9200-30f516a9b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79619756-a2f2-4195-bdc4-fbdaed6e8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.144.3  113.18 KiB  16      100.0%            787e8e70-a1bb-4420-bba4-36d04cb23f41  rack1\n",
      "UN  192.168.144.4  81.58 KiB   16      100.0%            1816a59e-9a47-4bdb-a23b-619e3bd689c1  rack1\n",
      "UN  192.168.144.2  149.48 KiB  16      100.0%            e1443135-7528-46df-858c-5738078c7f44  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312c6b09-9232-4e47-bde7-3a75f2d24579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415213e7-4ad1-43ec-b6f8-845004b319a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fb920721ff0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "cass.execute(\"\"\"\n",
    "    CREATE KEYSPACE weather\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3}\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TYPE weather.station_record (\n",
    "        tmin int,\n",
    "        tmax int\n",
    "    )\n",
    "\"\"\")\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TABLE weather.stations (\n",
    "    id text,\n",
    "    date date,\n",
    "    name text STATIC,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY (id, date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC);\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aca768e-830f-4f77-958a-12275963168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1: What is the Schema of stations?\n",
    "cass.execute(\"\"\"\n",
    "DESCRIBE TABLE weather.stations\n",
    "\"\"\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188b70f7-7ca1-4618-abe7-b6df51d0e70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b0c3b34b-b682-4df0-b7af-94f3a33cee99;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1240ms :: artifacts dl 68ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b0c3b34b-b682-4df0-b7af-94f3a33cee99\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/21ms)\n",
      "23/12/14 15:20:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d28646a-f9a7-4b06-a0df-527cf26b1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "df2 = df.withColumns({'station': expr(\"substring(value,1,11)\"), 'state': expr(\"substring(value,39,2)\"), 'name': expr(\"substring(value,42,30)\")})\n",
    "wisco_df = df2.where(col(\"state\") == \"WI\")\n",
    "wisco_list = wisco_df.collect()\n",
    "\n",
    "for row in wisco_list:\n",
    "    id = row['station'].strip()\n",
    "    name = row['name'].strip()\n",
    "    cass.execute(\"\"\"\n",
    "    INSERT INTO weather.stations (id, name)\n",
    "    VALUES (%s, %s)\n",
    "    \"\"\", (id, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26f8e1c-7f5e-48c1-8eae-bcd779838ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(count=1313)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "SELECT COUNT(*) FROM weather.stations\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f55a0b-fa20-45db-879e-d5a78fb07db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2: what is the name corresponding to station ID USW00014837?\n",
    "cass.execute(\"\"\"\n",
    "select name FROM weather.stations WHERE id = 'USW00014837'\n",
    "\"\"\").one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cc85f1-88c9-4466-8f00-dc4a3fa2332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3: what is the token for the USC00470273 station\n",
    "cass.execute(\"\"\"\n",
    "SELECT token(id), id FROM weather.stations WHERE id = 'USC00470273'\n",
    "\"\"\").one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cd3e97-1c1f-4a63-92e8-af68886ce8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8939171438556067363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4: what is the first vnode token in the ring following the token for USC00470273?\n",
    "import subprocess\n",
    "wi_quer = cass.execute(\"\"\"\n",
    "SELECT token(id), id FROM weather.stations WHERE id = 'USC00470273'\n",
    "\"\"\").one()[0]\n",
    "output = subprocess.check_output(['nodetool', 'ring']).decode('utf-8')\n",
    "vnode_tokens = []\n",
    "for line in output.split('\\n'):\n",
    "    parts = line.split()\n",
    "    if len(parts) > 7:\n",
    "        try:\n",
    "            vnode_tokens.append(int(parts[7]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "vnode_tokens.sort()\n",
    "next_token = None\n",
    "for token in vnode_tokens:\n",
    "    if token > wi_quer:\n",
    "        next_token = token\n",
    "        break\n",
    "\n",
    "if next_token is None and vnode_tokens:\n",
    "    next_token = vnode_tokens[0]\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7061e119-3981-4561-9557-39a3bf328207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 15:21:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rearranged DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 15:21:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+\n",
      "|    station|    date|  tmin|  tmax|\n",
      "+-----------+--------+------+------+\n",
      "|USW00014898|20220107|-166.0| -71.0|\n",
      "|USW00014839|20220924| 117.0| 194.0|\n",
      "|USW00014839|20220523|  83.0| 150.0|\n",
      "|USW00014839|20221019|  11.0|  83.0|\n",
      "|USW00014839|20220529| 139.0| 261.0|\n",
      "|USR0000WDDG|20221130|-106.0| -39.0|\n",
      "|USR0000WDDG|20220119|-178.0| -56.0|\n",
      "|USW00014837|20220222| -88.0| -38.0|\n",
      "|USR0000WDDG|20220202|-150.0|-106.0|\n",
      "|USW00014839|20220427|   0.0|  39.0|\n",
      "|USW00014839|20220708| 189.0| 222.0|\n",
      "|USW00014839|20220917| 200.0| 294.0|\n",
      "|USW00014837|20220624| 200.0| 322.0|\n",
      "|USW00014898|20220129|-116.0| -60.0|\n",
      "|USW00014839|20220715| 156.0| 233.0|\n",
      "|USR0000WDDG|20220224|-128.0| -61.0|\n",
      "|USR0000WDDG|20220130|-117.0| -33.0|\n",
      "|USR0000WDDG|20220414| -17.0|  50.0|\n",
      "|USW00014898|20220728| 156.0| 256.0|\n",
      "|USW00014837|20220906| 117.0| 256.0|\n",
      "+-----------+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Local file paths\n",
    "zip_file_path = \"records.zip\"\n",
    "extracted_dir_path = \"records.parquet\"\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir_path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WeatherData\").getOrCreate()\n",
    "\n",
    "parquet_path = os.path.join(extracted_dir_path, \"records.parquet\")\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "pivot_df = df.groupBy(\"station\", \"date\").pivot(\"element\").agg({\"value\": \"first\"})\n",
    "\n",
    "pivot_df = pivot_df.selectExpr(\n",
    "    \"station\",\n",
    "    \"date\",\n",
    "    \"`tmin` as tmin\",\n",
    "    \"`tmax` as tmax\"\n",
    ")\n",
    "\n",
    "print(\"Rearranged DataFrame:\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b700a40-89b6-45c4-aac1-2795ec827edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from server import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea676b7-25b2-4264-9d6f-3fcdb046a6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# servicer_instance = StationServicer()\n",
    "data_collect = pivot_df.collect()\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for row in data_collect:\n",
    "    request = station_pb2.RecordTempsRequest(\n",
    "    station=row[\"station\"],\n",
    "    date=row[\"date\"],\n",
    "    tmin=int(row[\"tmin\"]),\n",
    "    tmax=int(row[\"tmax\"])\n",
    ")\n",
    "    response = stub.RecordTemps(request)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da055e15-9ce4-45ea-860b-9ce26b89599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00479053</td>\n",
       "      <td>None</td>\n",
       "      <td>W BEND FIRE STN #2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00476398</td>\n",
       "      <td>None</td>\n",
       "      <td>PARK FALLS DNR HQ</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00470268</td>\n",
       "      <td>None</td>\n",
       "      <td>APPOLONIA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00474110</td>\n",
       "      <td>None</td>\n",
       "      <td>JUNEAU</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00475525</td>\n",
       "      <td>None</td>\n",
       "      <td>MINONG 5 WSW</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US1WIMN0013</td>\n",
       "      <td>None</td>\n",
       "      <td>TOMAH 7.5 SSW</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US1WIWK0039</td>\n",
       "      <td>None</td>\n",
       "      <td>OCONOMOWOC 0.6 N</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US1WIBR0019</td>\n",
       "      <td>None</td>\n",
       "      <td>RICE LAKE 5.8 N</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>USC00472626</td>\n",
       "      <td>None</td>\n",
       "      <td>EPHRAIM 1NE-WWTP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USC00477174</td>\n",
       "      <td>None</td>\n",
       "      <td>RIDGELAND 1NNE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  date                name record\n",
       "0  USC00479053  None  W BEND FIRE STN #2   None\n",
       "1  USC00476398  None   PARK FALLS DNR HQ   None\n",
       "2  USC00470268  None           APPOLONIA   None\n",
       "3  USC00474110  None              JUNEAU   None\n",
       "4  USC00475525  None        MINONG 5 WSW   None\n",
       "5  US1WIMN0013  None       TOMAH 7.5 SSW   None\n",
       "6  US1WIWK0039  None    OCONOMOWOC 0.6 N   None\n",
       "7  US1WIBR0019  None     RICE LAKE 5.8 N   None\n",
       "8  USC00472626  None    EPHRAIM 1NE-WWTP   None\n",
       "9  USC00477174  None      RIDGELAND 1NNE   None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(cass.execute(\"select * from weather.stations limit 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6524227-d7fa-437d-a8c1-37f59e26700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5: what is the max temperature ever seen for station USW00014837?\n",
    "request = station_pb2.StationMaxRequest(station='USW00014837')\n",
    "response = stub.StationMax(request)\n",
    "response.tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd7c181-a060-4aa8-b49b-5e895599b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+--------------------+\n",
      "|         id|date|record|                name|\n",
      "+-----------+----+------+--------------------+\n",
      "|US1WIMW0043|null|  null|   BROWN DEER 0.8 NW|\n",
      "|US1WIRC0016|null|  null|   WATERFORD 1.0 SSW|\n",
      "|US1WIOG0030|null|  null|     APPLETON 2.5 NW|\n",
      "|USC00470660|null|  null|          BELLEVILLE|\n",
      "|US1WIDA0062|null|  null|      WAUNAKEE 4.9 W|\n",
      "|USC00477182|null|  null|            RIDGEWAY|\n",
      "|US1WIOG0028|null|  null| BLACK CREEK 5.6 WNW|\n",
      "|US1WIWK0067|null|  null|    WAUKESHA 1.8 ENE|\n",
      "|USC00476392|null|  null|    PARDEEVILLE WWTP|\n",
      "|USC00474375|null|  null|   LA CROSSE WB CITY|\n",
      "|USC00478190|null|  null|STOCKBRIDGE-MUNSE...|\n",
      "|US1WIWK0085|null|  null|  BROOKFIELD 2.5 WNW|\n",
      "|USC00474295|null|  null|           KOEPENICK|\n",
      "|US1WIJF0003|null|  null|  LAKE MILLS 3.6 WNW|\n",
      "|US1WIBN0008|null|  null|       DE PERE 2.4 W|\n",
      "|US1WIDA0049|null|  null|      VERONA 5.5 WNW|\n",
      "|US1WIMW0028|null|  null|    FRANKLIN 2.4 ENE|\n",
      "|USC00470645|null|  null|     BEAVER DAM WWTP|\n",
      "|US1WISB0017|null|  null|HOWARDS GROVE 1.0...|\n",
      "|US1WIBT0025|null|  null|     WEBSTER 7.3 ESE|\n",
      "+-----------+----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stations_df = spark.read.format(\"org.apache.spark.sql.cassandra\") \\\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\") \\\n",
    ".option(\"keyspace\", \"weather\") \\\n",
    ".option(\"table\", \"stations\") \\\n",
    ".load()\n",
    "stations_df.createOrReplaceTempView(\"stations\")\n",
    "result = spark.sql(\"SELECT * FROM stations\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5477de1-9029-4f7f-942c-069d7589c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6: what tables/views are available in the Spark catalog?\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fca4f2e-4e6c-4b9c-a712-33e78a14d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7: what is the average difference between tmax and tmin, for each of the four stations that have temperature records?\n",
    "df_no_missing_values = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM stations\n",
    "    WHERE id IS NOT NULL AND date IS NOT NULL AND record IS NOT NULL\n",
    "\"\"\")\n",
    "df_no_missing_values.createOrReplaceTempView(\"no_na\")\n",
    "# df_no_na = result.na.drop()\n",
    "averages = spark.sql(\"SELECT id, AVG(record.tmax - record.tmin) AS AVG FROM no_na GROUP BY id\")\n",
    "q7_dict = {}\n",
    "for row in averages.collect():\n",
    "    q7_dict[row['id']] = row['AVG']\n",
    "q7_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f55b381-cb56-49e6-b3b9-bb3476ea4e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1214 15:22:21.167861038    1688 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2023-12-14T15:22:21.167618096+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2023-12-14T15:22:21.167499034+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 15:22:23 WARN ChannelPool: [s0|p6-db-2/192.168.144.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=2324e1e1-4d02-4a0b-9594-cfc367f68e32, APPLICATION_NAME=Spark-Cassandra-Connector-local-1702567241052}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.144.3  225.72 KiB  16      100.0%            787e8e70-a1bb-4420-bba4-36d04cb23f41  rack1\n",
      "UN  192.168.144.4  211.75 KiB  16      100.0%            1816a59e-9a47-4bdb-a23b-619e3bd689c1  rack1\n",
      "UN  192.168.144.2  248.35 KiB  16      100.0%            e1443135-7528-46df-858c-5738078c7f44  rack1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.connection:Heartbeat failed for connection (140433090021744) to 192.168.144.3:9042\n",
      "WARNING:cassandra.cluster:Host 192.168.144.3:9042 has been marked down\n",
      "23/12/14 15:22:30 WARN ChannelPool: [s0|p6-db-2/192.168.144.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=2324e1e1-4d02-4a0b-9594-cfc367f68e32, APPLICATION_NAME=Spark-Cassandra-Connector-local-1702567241052}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.144.3:9042, scheduling retry in 1.9 seconds: [Errno None] Tried connecting to [('192.168.144.3', 9042)]. Last error: timed out\n"
     ]
    }
   ],
   "source": [
    "#q8: what does nodetool status output?\n",
    "! nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "174e8481-a83f-4ba0-8e8d-b4ec056a5e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.144.3:9042, scheduling retry in 4.6 seconds: [Errno None] Tried connecting to [('192.168.144.3', 9042)]. Last error: timed out\n",
      "23/12/14 15:22:38 WARN ChannelPool: [s0|p6-db-2/192.168.144.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=2324e1e1-4d02-4a0b-9594-cfc367f68e32, APPLICATION_NAME=Spark-Cassandra-Connector-local-1702567241052}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=\"Operation timed out - received only 2 responses.\" info={'consistency': 'THREE', 'required_responses': 3, 'received_responses': 2}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m max_statement\u001b[38;5;241m.\u001b[39mconsistency_level \u001b[38;5;241m=\u001b[39m ConsistencyLevel\u001b[38;5;241m.\u001b[39mTHREE\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mcass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUSW00014839\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unavailable \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mrequired_replicas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m replicas, but only have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39malive_replicas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/cassandra/cluster.py:2637\u001b[0m, in \u001b[0;36mcassandra.cluster.Session.execute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/cassandra/cluster.py:4920\u001b[0m, in \u001b[0;36mcassandra.cluster.ResponseFuture.result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=\"Operation timed out - received only 2 responses.\" info={'consistency': 'THREE', 'required_responses': 3, 'received_responses': 2}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.144.3:9042, scheduling retry in 8.8 seconds: [Errno None] Tried connecting to [('192.168.144.3', 9042)]. Last error: timed out\n",
      "23/12/14 15:22:50 WARN ChannelPool: [s0|p6-db-2/192.168.144.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=2324e1e1-4d02-4a0b-9594-cfc367f68e32, APPLICATION_NAME=Spark-Cassandra-Connector-local-1702567241052}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    }
   ],
   "source": [
    "max_statement = cass.prepare(\"SELECT MAX(record.tmax) FROM weather.stations WHERE id = ?\")\n",
    "\n",
    "max_statement.consistency_level = ConsistencyLevel.THREE\n",
    "\n",
    "try:\n",
    "    resp = cass.execute(max_statement, ('USW00014839',))\n",
    "except Unavailable as e:\n",
    "    error_msg = f'need {e.required_replicas} replicas, but only have {e.alive_replicas}'\n",
    "    print(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806f8335-bb61-4dc6-be66-1951fde925f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need 3 replicas, but only have 2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.144.3:9042, scheduling retry in 16.32 seconds: [Errno None] Tried connecting to [('192.168.144.3', 9042)]. Last error: timed out\n"
     ]
    }
   ],
   "source": [
    "#q9: if you make a StationMax RPC call, what does the error field contain in StationMaxReply reply?\n",
    "request = station_pb2.StationMaxRequest(station='USR0000WDDG')\n",
    "response = stub.StationMax(request)\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a222c1ad-d7ff-4be6-84ae-db889317bc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 15:23:08 WARN ChannelPool: [s0|p6-db-2/192.168.144.3:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=2324e1e1-4d02-4a0b-9594-cfc367f68e32, APPLICATION_NAME=Spark-Cassandra-Connector-local-1702567241052}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.144.3:9042, scheduling retry in 29.76 seconds: [Errno 113] Tried connecting to [('192.168.144.3', 9042)]. Last error: No route to host\n"
     ]
    }
   ],
   "source": [
    "#q10: if you make a RecordTempsRequest RPC call, what does error contain in the RecordTempsReply reply?\n",
    "request = station_pb2.RecordTempsRequest(\n",
    "station='USW00014839',\n",
    "date='20220917',\n",
    "tmin=int(-36),\n",
    "tmax=int(-6)\n",
    ")\n",
    "response = stub.RecordTemps(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3813bf-2942-45c9-91c0-a9087c65e120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
